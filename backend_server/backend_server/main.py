import os
import cv2
import json
import base64
from openai import OpenAI
from local_langchain import run_pipeline
from movie import render_shorts_from_timeline
from dotenv import load_dotenv

# ==============================
# 0. ì„¤ì •
# ==============================
MEDIA_DIR = "media"
RESULT_DIR = "results"
os.makedirs(RESULT_DIR, exist_ok=True)

# í™˜ê²½ë³€ìˆ˜ë‚˜ ì§ì ‘ API Key ì§€ì •
load_dotenv()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# ==============================
# 1. OpenAI Vision ì´ë¯¸ì§€ ë¶„ì„
# ==============================
def analyze_image_openai(image_path: str) -> dict:
    print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘: {image_path}")

    with open(image_path, "rb") as f:
        img_b64 = base64.b64encode(f.read()).decode("utf-8")

    prompt = (
        "ì´ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ë¶„ì„í•´ì¤˜. "
        "ë¬´ì—‡ì´ ë³´ì´ëŠ”ì§€, ì‚¬ëŒ/ì‚¬ë¬¼/í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³ , "
        "ì´ë¯¸ì§€ì˜ ì „ì²´ì ì¸ ë¶„ìœ„ê¸°ë‚˜ ìƒí™©ì„ ìš”ì•½í•´ì¤˜."
    )

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},  # âœ… ìˆ˜ì •: input_text â†’ text
                        {
                            "type": "image_url",  # âœ… ìˆ˜ì •: input_image â†’ image_url
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{img_b64}"
                                    }
                        },
                    ],
                }
            ],
            temperature=0.2,
        )

        description = response.choices[0].message.content
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {os.path.basename(image_path)}")
        return {"type": "image", "filename": os.path.basename(image_path), "description": description}
    except Exception as e:
        print(f"âš ï¸ ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {"type": "image", "filename": os.path.basename(image_path), "description": None}


# ==============================
# 2. OpenAI Vision ê¸°ë°˜ ì˜ìƒ ë¶„ì„
# ==============================
def analyze_video_openai(video_path: str, num_frames: int = 5) -> dict:
    print(f"ğŸ¥ ì˜ìƒ ë¶„ì„ ì¤‘: {video_path}")

    cap = cv2.VideoCapture(video_path)
    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frames = []

    for i in range(num_frames):
        idx = int(total * (i + 1) / (num_frames + 1))
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if not ret:
            continue
        _, buf = cv2.imencode(".jpg", frame)
        b64 = base64.b64encode(buf).decode("utf-8")
        frames.append(b64)
    cap.release()

    prompt = (
        "ì´ ì˜ìƒì˜ ì¥ë©´ ë³€í™”ì™€ ë™ì‘ì„ ì„¤ëª…í•´ì¤˜. "
        "í”„ë ˆì„ ê°„ì˜ ì›€ì§ì„, ë“±ì¥í•˜ëŠ” ì¸ë¬¼/ì‚¬ë¬¼, ì „í™˜ëœ ì¥ë©´ë“¤ ìš”ì•½í•´ì¤˜."
    )

    try:
        content = [{"type": "text", "text": prompt}]
        for b64 in frames:
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/jpeg;base64,{b64}"}
            })

        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": content}],
            temperature=0.2,
        )
        description = response.choices[0].message.content
        print(f"âœ… ì˜ìƒ ë¶„ì„ ì™„ë£Œ: {os.path.basename(video_path)}")
        return {"type": "video", "filename": os.path.basename(video_path), "description": description}
    except Exception as e:
        print(f"âš ï¸ ì˜ìƒ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return {"type": "video", "filename": os.path.basename(video_path), "description": None}



# ==============================
# 3. ëª¨ë“  ë¯¸ë””ì–´ íŒŒì¼ í†µí•© ë¶„ì„
# ==============================
def analyze_all_media() -> list:
    print("ğŸ“‚ media í´ë”ì—ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")
    files = [
        os.path.join(MEDIA_DIR, f)
        for f in os.listdir(MEDIA_DIR)
        if f.lower().endswith((".jpg", ".jpeg", ".png", ".mp4"))
    ]

    print(f"ğŸ“¦ ì´ {len(files)}ê°œì˜ íŒŒì¼ ê°ì§€ë¨")
    all_results = []

    for path in files:
        if path.lower().endswith((".jpg", ".jpeg", ".png")):
            result = analyze_image_openai(path)
        else:
            result = analyze_video_openai(path)
        all_results.append(result)

    output_path = os.path.join(RESULT_DIR, "analysis_result.json")
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    print(f"âœ… ëª¨ë“  ë¯¸ë””ì–´ ë¶„ì„ ì™„ë£Œ â†’ {output_path}")
    return all_results

def normalize_openai_analysis(openai_results: list, user_prompt: str) -> dict:
    """OpenAI Vision ë¶„ì„ ë¦¬ìŠ¤íŠ¸ë¥¼ LangChain íŒŒì´í”„ë¼ì¸ìš© dictë¡œ ë³€í™˜"""
    videos, images, audio = [], [], []

    for item in openai_results:
        if item.get("type") == "video":
            videos.append({
                "filename": item["filename"],
                "description": item.get("description", "")
            })
        elif item.get("type") == "image":
            images.append({
                "filename": item["filename"],
                "description": item.get("description", "")
            })
        elif item.get("type") == "audio":
            audio.append({
                "filename": item["filename"],
                "description": item.get("description", "")
            })

    return {
        "videos": videos,
        "images": images,
        "audio": audio,
        "user_prompt": user_prompt
    }



# ==============================
# 4. LangChain + MoviePy í†µí•© íŒŒì´í”„ë¼ì¸
# ==============================
def main():
    print("ğŸš€ OpenAI Vision ê¸°ë°˜ í†µí•© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘!")
    combined_analysis = analyze_all_media()
    
    prompt = "ë‚´ê°€ ì§€ê¸ˆ í•œêµ­ í´ë¦¬í… ëŒ€í•™ AIìœµí•©ì†Œí”„íŠ¸ì›¨ì–´ê³¼ì— ëŒ€í•œ í•™ê³¼ ì†Œê°œ ì˜ìƒì„ ë§Œë“¤ê³  ì‹¶ì–´. í•™ê³¼ì˜ íŠ¹ì§•ê³¼ ì¥ì ì— ëŒ€í•´ì„œ ì•Œë ¤ì£¼ëŠ” ì˜ìƒì„ ë§Œë“¤ì–´ì¤˜"
    print("\nğŸ§  LangChain íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...")
        
        
        # âœ… analysis_result.jsonì—ì„œ ë¶„ì„ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°
    with open(os.path.join(RESULT_DIR, "analysis_result.json"), "r", encoding="utf-8") as f:
        combined_analysis = json.load(f)

    
    
    combined_analysis = normalize_openai_analysis(combined_analysis, prompt)
    result = run_pipeline(combined_analysis, duration=30, user_prompt = prompt)

    # Pydantic-safe JSON ë³€í™˜
    def make_json_serializable(obj):
        result = run_pipeline(combined_analysis, duration=30, user_prompt=prompt)

# âœ… LangChain Pydantic ê²°ê³¼ë¥¼ JSON/dictë¡œ ë³€í™˜
    if hasattr(result, "model_dump"):
        result = result.model_dump()
    elif isinstance(result, dict):
        # ë‚´ë¶€ Pydantic ê°ì²´ê¹Œì§€ ì•ˆì „ ë³€í™˜
        def recursive_dump(obj):
            if hasattr(obj, "model_dump"):
                return obj.model_dump()
            elif isinstance(obj, dict):
                return {k: recursive_dump(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [recursive_dump(v) for v in obj]
            else:
                return obj
        result = recursive_dump(result)

    print("\nğŸ¬ MoviePy ì˜ìƒ ë Œë”ë§ ì¤‘...")
    render_shorts_from_timeline(result, output_path=os.path.join(RESULT_DIR, "final_shorts.mp4"))

    
    
    
    print("âœ… ìµœì¢… ì˜ìƒ ìƒì„± ì™„ë£Œ!")



# ==============================
# 5. ì‹¤í–‰ ì§„ì…ì 
# ==============================
if __name__ == "__main__":
    main()
